{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_doc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrijain/NLP-/blob/master/Task6%20tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Q9MiBXX5pOzW",
        "colab_type": "code",
        "outputId": "f7e2787e-efbc-4745-cbad-3f285917aef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "text = ' Just returned from France where much was accomplished in my meetings with World Leaders. Never easy bringing up the fact that the U.S. must be treated fairly, which it hasn’t, on both Military and Trade. We pay for LARGE portions of other countries military protection'\n",
        "twtkn = TweetTokenizer()\n",
        "twtkn.tokenize(text)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'returned',\n",
              " 'from',\n",
              " 'France',\n",
              " 'where',\n",
              " 'much',\n",
              " 'was',\n",
              " 'accomplished',\n",
              " 'in',\n",
              " 'my',\n",
              " 'meetings',\n",
              " 'with',\n",
              " 'World',\n",
              " 'Leaders',\n",
              " '.',\n",
              " 'Never',\n",
              " 'easy',\n",
              " 'bringing',\n",
              " 'up',\n",
              " 'the',\n",
              " 'fact',\n",
              " 'that',\n",
              " 'the',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.',\n",
              " 'must',\n",
              " 'be',\n",
              " 'treated',\n",
              " 'fairly',\n",
              " ',',\n",
              " 'which',\n",
              " 'it',\n",
              " 'hasn',\n",
              " '’',\n",
              " 't',\n",
              " ',',\n",
              " 'on',\n",
              " 'both',\n",
              " 'Military',\n",
              " 'and',\n",
              " 'Trade',\n",
              " '.',\n",
              " 'We',\n",
              " 'pay',\n",
              " 'for',\n",
              " 'LARGE',\n",
              " 'portions',\n",
              " 'of',\n",
              " 'other',\n",
              " 'countries',\n",
              " 'military',\n",
              " 'protection']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "OKRFwaCsYkWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfde95cd-b2f7-4571-969d-486aa6303f9a"
      },
      "cell_type": "code",
      "source": [
        "# This program tokenizes text by spaces\n",
        "\n",
        "import nltk\n",
        "text = \"this is john's text, isn't it?\"\n",
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', \"john's\", 'text,', \"isn't\", 'it?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "kkFGqvlXpeLm",
        "colab_type": "code",
        "outputId": "11df7dfd-4797-4474-a8a0-e599aa44906a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# This program tokenizes text by words\n",
        "\n",
        "import nltk\n",
        "text = \"this is john's text, isn't it?\"\n",
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'john', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "Qx1kjtq8pteU",
        "colab_type": "code",
        "outputId": "edaa007b-9c98-4f30-b69d-dff801b11466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# This program tokenizes text by punctuations\n",
        "\n",
        "import nltk\n",
        "text = \"this is john's text, isn't it?\"\n",
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', 'john', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}